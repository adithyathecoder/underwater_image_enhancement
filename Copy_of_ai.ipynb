{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyathecoder/underwater_image_enhancement/blob/main/Copy_of_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDEgowXznzVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399f3112-03cd-4bca-858c-f7f9130ebc9d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJA8nDFv09kD"
      },
      "source": [
        "%%capture\n",
        "!pip install timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd0XW4mA1Dm4"
      },
      "source": [
        "import torch as T\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import cv2\n",
        "from PIL import Image, ImageOps, ImageEnhance, __version__ as PILLOW_VERSION\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, SequentialSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as tvf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "from prettytable import PrettyTable\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = \"cuda\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMiXPlNu1U-L"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, backbone = 'resnet18', device = 'cuda'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.backbone = timm.create_model(backbone, pretrained = True)\n",
        "        self.List = list(self.backbone.children())[:-2]\n",
        "        self.device = device\n",
        "    def forward(self,X):\n",
        "        X = X.to(self.device).float()\n",
        "        outputs = []\n",
        "        for i,layer in enumerate(self.List):\n",
        "            X = layer(X)\n",
        "            if i > 1:\n",
        "                outputs.append(X)\n",
        "        return outputs\n",
        "\n",
        "class ResidualLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels):\n",
        "        super(ResidualLayer, self).__init__()\n",
        "        self.resblock = nn.Sequential(nn.Conv2d(in_channels, out_channels,\n",
        "                                                kernel_size=3, padding=1, bias=False),\n",
        "                                      nn.ReLU(True),\n",
        "                                      nn.Conv2d(out_channels, out_channels,\n",
        "                                                kernel_size=1, bias=False))\n",
        "        self.conv = nn.Conv2d(in_channels*2, out_channels,\n",
        "                                                kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, input_1,input_2):\n",
        "        Y = input_1 + self.resblock(input_1)\n",
        "        Y = T.cat((Y,input_2),dim=1)\n",
        "        return self.conv(Y)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode = 'bilinear')\n",
        "        self.res1 = ResidualLayer(256,256)\n",
        "        self.res2 = ResidualLayer(128,128)\n",
        "        self.res3 = ResidualLayer(64,64)\n",
        "        self.res4 = ResidualLayer(64,64)\n",
        "        self.conv1 = nn.Conv2d(512,256,(3,3),padding = 1)  \n",
        "        self.conv2 = nn.Conv2d(256,128,(3,3),padding = 1)  \n",
        "        self.conv3 = nn.Conv2d(128,64,(3,3),padding = 1)\n",
        "        self.conv4 = nn.Conv2d(64,64,(3,3),padding = 1)\n",
        "        self.out = nn.Conv2d(64,3,(1,1))\n",
        "        \n",
        "    def forward(self,outputs):\n",
        "        X = self.upsample(outputs[-1])\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = self.res1(X,outputs[-2])\n",
        "        X = self.upsample(X)\n",
        "        X = F.relu(self.conv2(X))\n",
        "        X = self.res2(X,outputs[-3])\n",
        "        X = self.upsample(X)\n",
        "        X = F.relu(self.conv3(X))\n",
        "        X = self.res3(X,outputs[-5])\n",
        "        X = self.upsample(X)\n",
        "        X = F.relu(self.conv4(X))\n",
        "        X = self.res4(X,outputs[-6])\n",
        "        return self.out(X)\n",
        "    \n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self,device='cuda'):\n",
        "        super(AutoEncoder,self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.to(device)\n",
        "        \n",
        "    def forward(self,X):\n",
        "        X = self.encoder(X)\n",
        "        X = self.decoder(X)\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chQyxXOd1lrE"
      },
      "source": [
        "train_dataset='/content/drive/MyDrive/Induction_training_files/raw'\n",
        "test_dataset='/content/drive/MyDrive/Induction_training_files/test'\n",
        "enhanced_dataset='/content/drive/MyDrive/Induction_training_files/enhanced'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqlhNKUxXb6e",
        "outputId": "8febcfba-5da8-4ae6-91a5-13cef126a6b3"
      },
      "source": [
        "import os\n",
        "a=os.listdir('/content/drive/MyDrive/Induction_training_files/raw')\n",
        "print(a)\n",
        "file_paths = []\n",
        "\n",
        "import os\n",
        "\n",
        "def get_filepaths(directory):\n",
        "    \n",
        "    file_paths = []  \n",
        "\n",
        "    # Walk the tree.\n",
        "    for root, directories, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            \n",
        "            filepath = os.path.join(root, filename)\n",
        "            file_paths.append(filepath)  \n",
        "\n",
        "    return file_paths  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['704_img_.png', '101_img_.png', '102_img_.png', '104_img_.png', '106_img_.png', '105_img_.png', '108_img_.png', '107_img_.png', '113_img_.png', '118_img_.png', '117_img_.png', '116_img_.png', '120_img_.png', '121_img_.png', '122_img_.png', '124_img_.png', '125_img_.png', '123_img_.png', '133_img_.png', '131_img_.png', '130_img_.png', '135_img_.png', '136_img_.png', '137_img_.png', '138_img_.png', '141_img_.png', '145_img_.png', '144_img_.png', '149_img_.png', '148_img_.png', '146_img_.png', '151_img_.png', '150_img_.png', '152_img_.png', '157_img_.png', '154_img_.png', '155_img_.png', '153_img_.png', '156_img_.png', '158_img_.png', '159_img_.png', '161_img_.png', '162_img_.png', '163_img_.png', '164_img_.png', '165_img_.png', '166_img_.png', '169_img_.png', '171_img_.png', '170_img_.png', '174_img_.png', '172_img_.png', '175_img_.png', '177_img_.png', '176_img_.png', '178_img_.png', '180_img_.png', '179_img_.png', '183_img_.png', '185_img_.png', '187_img_.png', '188_img_.png', '189_img_.png', '192_img_.png', '190_img_.png', '191_img_.png', '193_img_.png', '195_img_.png', '196_img_.png', '194_img_.png', '197_img_.png', '198_img_.png', '200_img_.png', '199_img_.png', '204_img_.png', '201_img_.png', '202_img_.png', '207_img_.png', '208_img_.png', '206_img_.png', '205_img_.png', '212_img_.png', '210_img_.png', '213_img_.png', '211_img_.png', '209_img_.png', '214_img_.png', '216_img_.png', '215_img_.png', '217_img_.png', '219_img_.png', '220_img_.png', '221_img_.png', '224_img_.png', '225_img_.png', '223_img_.png', '226_img_.png', '228_img_.png', '231_img_.png', '230_img_.png', '233_img_.png', '232_img_.png', '234_img_.png', '238_img_.png', '236_img_.png', '241_img_.png', '243_img_.png', '245_img_.png', '242_img_.png', '248_img_.png', '246_img_.png', '247_img_.png', '252_img_.png', '251_img_.png', '253_img_.png', '255_img_.png', '257_img_.png', '254_img_.png', '260_img_.png', '259_img_.png', '258_img_.png', '266_img_.png', '262_img_.png', '263_img_.png', '261_img_.png', '265_img_.png', '270_img_.png', '268_img_.png', '267_img_.png', '272_img_.png', '275_img_.png', '271_img_.png', '273_img_.png', '276_img_.png', '278_img_.png', '277_img_.png', '280_img_.png', '282_img_.png', '281_img_.png', '285_img_.png', '288_img_.png', '284_img_.png', '292_img_.png', '290_img_.png', '293_img_.png', '296_img_.png', '295_img_.png', '299_img_.png', '294_img_.png', '301_img_.png', '302_img_.png', '300_img_.png', '307_img_.png', '311_img_.png', '305_img_.png', '306_img_.png', '303_img_.png', '304_img_.png', '312_img_.png', '315_img_.png', '320_img_.png', '314_img_.png', '313_img_.png', '317_img_.png', '322_img_.png', '318_img_.png', '319_img_.png', '316_img_.png', '327_img_.png', '326_img_.png', '323_img_.png', '324_img_.png', '328_img_.png', '325_img_.png', '336_img_.png', '332_img_.png', '338_img_.png', '330_img_.png', '340_img_.png', '334_img_.png', '337_img_.png', '341_img_.png', '346_img_.png', '342_img_.png', '343_img_.png', '347_img_.png', '351_img_.png', '355_img_.png', '353_img_.png', '350_img_.png', '348_img_.png', '356_img_.png', '359_img_.png', '360_img_.png', '362_img_.png', '367_img_.png', '361_img_.png', '364_img_.png', '366_img_.png', '363_img_.png', '371_img_.png', '369_img_.png', '370_img_.png', '372_img_.png', '374_img_.png', '375_img_.png', '373_img_.png', '377_img_.png', '376_img_.png', '379_img_.png', '380_img_.png', '378_img_.png', '381_img_.png', '382_img_.png', '384_img_.png', '383_img_.png', '386_img_.png', '385_img_.png', '387_img_.png', '391_img_.png', '389_img_.png', '392_img_.png', '393_img_.png', '394_img_.png', '395_img_.png', '407_img_.png', '404_img_.png', '406_img_.png', '405_img_.png', '409_img_.png', '410_img_.png', '408_img_.png', '412_img_.png', '413_img_.png', '418_img_.png', '419_img_.png', '417_img_.png', '420_img_.png', '422_img_.png', '421_img_.png', '423_img_.png', '428_img_.png', '425_img_.png', '424_img_.png', '432_img_.png', '429_img_.png', '435_img_.png', '436_img_.png', '433_img_.png', '437_img_.png', '438_img_.png', '439_img_.png', '440_img_.png', '444_img_.png', '442_img_.png', '443_img_.png', '441_img_.png', '447_img_.png', '445_img_.png', '446_img_.png', '451_img_.png', '450_img_.png', '449_img_.png', '457_img_.png', '452_img_.png', '455_img_.png', '454_img_.png', '453_img_.png', '458_img_.png', '459_img_.png', '461_img_.png', '460_img_.png', '462_img_.png', '464_img_.png', '466_img_.png', '467_img_.png', '465_img_.png', '469_img_.png', '470_img_.png', '471_img_.png', '479_img_.png', '478_img_.png', '476_img_.png', '477_img_.png', '483_img_.png', '482_img_.png', '480_img_.png', '484_img_.png', '487_img_.png', '486_img_.png', '485_img_.png', '489_img_.png', '493_img_.png', '492_img_.png', '491_img_.png', '494_img_.png', '497_img_.png', '501_img_.png', '502_img_.png', '503_img_.png', '504_img_.png', '505_img_.png', '509_img_.png', '508_img_.png', '510_img_.png', '511_img_.png', '512_img_.png', '519_img_.png', '516_img_.png', '517_img_.png', '518_img_.png', '515_img_.png', '526_img_.png', '530_img_.png', '528_img_.png', '527_img_.png', '535_img_.png', '533_img_.png', '532_img_.png', '534_img_.png', '537_img_.png', '538_img_.png', '536_img_.png', '539_img_.png', '542_img_.png', '545_img_.png', '544_img_.png', '543_img_.png', '541_img_.png', '546_img_.png', '540_img_.png', '550_img_.png', '549_img_.png', '551_img_.png', '557_img_.png', '556_img_.png', '555_img_.png', '558_img_.png', '554_img_.png', '552_img_.png', '560_img_.png', '561_img_.png', '562_img_.png', '563_img_.png', '564_img_.png', '570_img_.png', '568_img_.png', '569_img_.png', '566_img_.png', '571_img_.png', '567_img_.png', '577_img_.png', '572_img_.png', '574_img_.png', '575_img_.png', '576_img_.png', '573_img_.png', '581_img_.png', '579_img_.png', '578_img_.png', '582_img_.png', '580_img_.png', '585_img_.png', '584_img_.png', '586_img_.png', '583_img_.png', '587_img_.png', '589_img_.png', '588_img_.png', '592_img_.png', '593_img_.png', '591_img_.png', '590_img_.png', '596_img_.png', '594_img_.png', '597_img_.png', '595_img_.png', '600_img_.png', '601_img_.png', '598_img_.png', '599_img_.png', '603_img_.png', '606_img_.png', '602_img_.png', '605_img_.png', '604_img_.png', '609_img_.png', '607_img_.png', '608_img_.png', '613_img_.png', '614_img_.png', '615_img_.png', '612_img_.png', '620_img_.png', '619_img_.png', '617_img_.png', '618_img_.png', '616_img_.png', '625_img_.png', '621_img_.png', '626_img_.png', '623_img_.png', '627_img_.png', '629_img_.png', '628_img_.png', '635_img_.png', '633_img_.png', '631_img_.png', '630_img_.png', '632_img_.png', '634_img_.png', '638_img_.png', '639_img_.png', '637_img_.png', '636_img_.png', '640_img_.png', '642_img_.png', '645_img_.png', '641_img_.png', '647_img_.png', '649_img_.png', '648_img_.png', '646_img_.png', '653_img_.png', '652_img_.png', '654_img_.png', '655_img_.png', '656_img_.png', '657_img_.png', '658_img_.png', '659_img_.png', '668_img_.png', '666_img_.png', '663_img_.png', '670_img_.png', '673_img_.png', '672_img_.png', '685_img_.png', '687_img_.png', '686_img_.png', '688_img_.png', '689_img_.png', '690_img_.png', '692_img_.png', '697_img_.png', '696_img_.png', '695_img_.png', '701_img_.png', '702_img_.png', '703_img_.png', '706_img_.png', '707_img_.png', '712_img_.png', '711_img_.png', '713_img_.png', '715_img_.png', '716_img_.png', '714_img_.png', '718_img_.png', '719_img_.png', '727_img_.png', '733_img_.png', '734_img_.png', '745_img_.png', '743_img_.png', '76_img_.png', '769_img_.png', '775_img_.png', '776_img_.png', '777_img_.png', '778_img_.png', '779_img_.png', '780_img_.png', '781_img_.png', '786_img_.png', '789_img_.png', '799_img_.png', '807_img_.png', '80_img_.png', '811_img_.png', '812_img_.png', '813_img_.png', '814_img_.png', '816_img_.png', '817_img_.png', '818_img_.png', '820_img_.png', '819_img_.png', '822_img_.png', '82_img_.png', '834_img_.png', '837_img_.png', '841_img_.png', '843_img_.png', '842_img_.png', '845_img_.png', '844_img_.png', '846_img_.png', '850_img_.png', '849_img_.png', '848_img_.png', '851_img_.png', '852_img_.png', '856_img_.png', '860_img_.png', '857_img_.png', '865_img_.png', '862_img_.png', '863_img_.png', '867_img_.png', '866_img_.png', '869_img_.png', '868_img_.png', '872_img_.png', '871_img_.png', '873_img_.png', '874_img_.png', '875_img_.png', '879_img_.png', '878_img_.png', '876_img_.png', '87_img_.png', '877_img_.png', '889_img_.png', '880_img_.png', '887_img_.png', '881_img_.png', '890_img_.png', '891_img_.png', '895_img_.png', '893_img_.png', '892_img_.png', '896_img_.png', '89_img_.png', '904_img_.png', '905_img_.png', '901_img_.png', '907_img_.png', '906_img_.png', '909_img_.png', '908_img_.png', '90_img_.png', '911_img_.png', '910_img_.png', '912_img_.png', '914_img_.png', '917_img_.png', '915_img_.png', '916_img_.png', '913_img_.png', '918_img_.png', '921_img_.png', '920_img_.png', '919_img_.png', '927_img_.png', '925_img_.png', '923_img_.png', '922_img_.png', '924_img_.png', '926_img_.png', '92_img_.png', '928_img_.png', '929_img_.png', '98_img_.png', '95_img_.png', '97_img_.png', '96_img_.png', '7027.png', '7643.png', '7654.png', '10151.png', '11052.png', '10945.png', '11064.png', '10139.png', '10909.png', '11374.png', '11398.png', '1225.png', '12290.png', '12299.png', '12348.png', '12336.png', '12324.png', '12422.png', '12433.png', '1407.png', '12900.png', '15003.png', '15001.png', '1491.png', '15045.png', '15094.png', '15136.png', '15113.png', '1539.png', '15704.png', '15544.png', '15418.png', '15426.png', '1660.png', '1742.png', '1573.png', '1957.png', '2546.png', '2677.png', '2701.png', '2629.png', '2787.png', '2774.png', '2882.png', '3015.png', '2977.png', '3196.png', '3001.png', '3202.png', '3330.png', '3650.png', '4070.png', '3728.png', '3947.png', '3925.png', '5554.png', '557.png', '539.png', '559.png', '5818.png', '5830.png', '6082.png', '563.png', '567.png', '6062.png', '598.png', '6788.png', '6872.png', '6820.png', '651.png', '758.png', '756.png', '7916.png', '8010.png', '8056.png', '8046.png', '8262.png', '8288.png', '837.png', '841.png', '9547.png', '9554.png', '9557.png', '9567.png', '890.png', '9907.png', '9947.png', '9900.png', '9896.png', '100_img_.png', '109_img_.png', '111_img_.png', '112_img_.png', '114_img_.png', '12445.png', '115_img_.png', '129_img_.png', '139_img_.png', '140_img_.png', '143_img_.png', '168_img_.png', '160_img_.png', '182_img_.png', '184_img_.png', '173_img_.png', '222_img_.png', '186_img_.png', '203_img_.png', '229_img_.png', '274_img_.png', '244_img_.png', '279_img_.png', '287_img_.png', '365_img_.png', '345_img_.png', '411_img_.png', '400_img_.png', '401_img_.png', '403_img_.png', '415_img_.png', '416_img_.png', '468_img_.png', '448_img_.png', '473_img_.png', '481_img_.png', '495_img_.png', '488_img_.png', '496_img_.png', '498_img_.png', '490_img_.png', '499_img_.png', '506_img_.png', '507_img_.png', '513_img_.png', '524_img_.png', '531_img_.png', '610_img_.png', '611_img_.png', '547_img_.png', '622_img_.png', '650_img_.png', '651_img_.png', '660_img_.png', '662_img_.png', '679_img_.png', '677_img_.png', '678_img_.png', '676_img_.png', '680_img_.png', '683_img_.png', '681_img_.png', '682_img_.png', '694_img_.png', '684_img_.png', '698_img_.png', '691_img_.png', '705_img_.png', '699_img_.png', '708_img_.png', '700_img_.png', '710_img_.png', '709_img_.png', '720_img_.png', '721_img_.png', '723_img_.png', '726_img_.png', '724_img_.png', '725_img_.png', '736_img_.png', '737_img_.png', '735_img_.png', '740_img_.png', '738_img_.png', '746_img_.png', '744_img_.png', '741_img_.png', '747_img_.png', '752_img_.png', '751_img_.png', '754_img_.png', '756_img_.png', '753_img_.png', '760_img_.png', '762_img_.png', '764_img_.png', '765_img_.png', '768_img_.png', '770_img_.png', '771_img_.png', '773_img_.png', '77_img_.png', '784_img_.png', '787_img_.png', '785_img_.png', '78_img_.png', '788_img_.png', '791_img_.png', '794_img_.png', '793_img_.png', '79_img_.png', '802_img_.png', '804_img_.png', '810_img_.png', '809_img_.png', '808_img_.png', '806_img_.png', '821_img_.png', '815_img_.png', '827_img_.png', '823_img_.png', '828_img_.png', '829_img_.png', '831_img_.png', '833_img_.png', '832_img_.png', '835_img_.png', '858_img_.png', '847_img_.png', '854_img_.png', '859_img_.png', '838_img_.png', '855_img_.png', '861_img_.png', '86_img_.png', '99_img_.png', '91_img_.png', '94_img_.png', '227_img_.png', '298_img_.png', '661_img_.png', '674_img_.png', '675_img_.png', '722_img_.png', '728_img_.png', '729_img_.png', '730_img_.png', '731_img_.png', '748_img_.png', '732_img_.png', '749_img_.png', '750_img_.png', '758_img_.png', '757_img_.png', '759_img_.png', '763_img_.png', '766_img_.png', '767_img_.png', '772_img_.png', '782_img_.png', '783_img_.png', '790_img_.png', '792_img_.png', '795_img_.png', '796_img_.png', '797_img_.png', '798_img_.png', '800_img_.png', '803_img_.png', '402_img_.png', '142_img_.png', '463_img_.png', '472_img_.png', '240_img_.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1lM9J2VknEi"
      },
      "source": [
        " path_test= get_filepaths(\"/content/drive/MyDrive/Induction_training_files/raw\")\n",
        " import pandas as pd\n",
        " import tensorflow as tf\n",
        " from tensorflow import keras\n",
        " from __future__ import absolute_import, division, print_function\n",
        " df= pd.DataFrame({\"raw\":path_test}) \n",
        " df.to_csv(\"small .csv\")\n",
        " path_enhance = get_filepaths(\"/content/drive/MyDrive/Induction_training_files/enhanced\")\n",
        " dp = pd.DataFrame({\"raw\":path_enhance})\n",
        " dp.to_csv(\"large .csv\")\n",
        " #indata = pd.read_csv('small .csv')\n",
        " #zipped = pd.DataFrame({\"enhance\":path_enhance})\n",
        " #axis=1 indicates to concat the frames column wise\n",
        " #outdata = pd.concat([indata, zipped], axis=1)\n",
        " #we dont want headers and dont want the row labels\n",
        " #outdata.to_csv('small .csv',index=False)\n",
        "checkpoint_path = '/content/model2.pth'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAWK_gsRsU6c"
      },
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, dir_=None, input_size=(224,224), output_size=(112, 112),prob = 0.15):\n",
        "        super().__init__()\n",
        "        self.directory = dir_\n",
        "        self.dataset = pd.read_csv(self.directory)\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.prob = prob\n",
        "        self.input_size_x = self.input_size[0]\n",
        "        self.input_size_y = self.input_size[1]\n",
        "        self.MODEL_SCALE = self.input_size[0]//self.output_size[0]\n",
        "        self.preprocess = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.dataset)\n",
        "\n",
        "    def __getitem_internal__(self, idx, preprocess=True):\n",
        "        target = self.dataset.iloc[idx]\n",
        "        noise_image= cv2.imread(target[\"raw\"])\n",
        "        noise_image = cv2.resize(noise_image,self.input_size)\n",
        "        noise_image = transforms.ToTensor()(np.array(noise_image))\n",
        "        dirname, basename = os.path.split(target[\"raw\"])\n",
        "        main,raws=os.path.split(dirname)\n",
        "        path = os.path.join(main, 'enhanced', basename)\n",
        "        rgb_image = cv2.imread(path) \n",
        "        rgb_image = cv2.resize(rgb_image,self.output_size)\n",
        "        rgb_image = transforms.ToTensor()(np.array(rgb_image))\n",
        "        return (noise_image,rgb_image)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.__getitem_internal__(idx, True)\n",
        "    \n",
        "    def raw(self, idx):\n",
        "        return self.__getitem_internal__(idx, False)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4eKBz6TtskR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c80cbcc-eb25-4621-a2db-47a5d34b1644"
      },
      "source": [
        "#import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#dataset_url = '/content/small .csv'\n",
        "#data = pd.read_csv(dataset_url)\n",
        "#y = data.raw\n",
        "#y_train, y_test = train_test_split(y,test_size=0.2)\n",
        "train_dataloader = Data('/content/small .csv')\n",
        "print(\"Train :\",train_dataloader.__len__())\n",
        "val_dataloader = Data('/content/large .csv')\n",
        "print(\"Train :\",val_dataloader.__len__())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train : 830\n",
            "Train : 830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHQgqMKXI6Fy"
      },
      "source": [
        "model = AutoEncoder()\n",
        "criterion1 = nn.L1Loss()\n",
        "criterion2 = nn.MSELoss()\n",
        "\n",
        "def loss_fn(predicted,target):\n",
        "    return criterion2(predicted,target)\n",
        "\n",
        "@T.no_grad()\n",
        "def validation(model, loader, loss_fn):\n",
        "    vlosses = []\n",
        "    v = tqdm(loader)\n",
        "    model.eval()\n",
        "    for i,(input_image,gt_image) in enumerate(v):\n",
        "        input_image,gt_image = input_image.to(device), gt_image.to(device)\n",
        "        y_pred = model(input_image)\n",
        "        vloss = loss_fn(y_pred,gt_image)\n",
        "        vlosses.append(vloss.item())\n",
        "    return np.array(vlosses).mean()\n",
        "    \n",
        "model.load_state_dict(T.load(\"/content/model2.pth\",map_location=T.device('cuda')))\n",
        "optimizer = T.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=3,verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLDuJ-iO_Dc_"
      },
      "source": [
        "batch_size = 30\n",
        "EPOCHES = 30\n",
        "train_loader = DataLoader(train_dataloader,batch_size=batch_size,shuffle=False, num_workers=0, sampler=SubsetRandomSampler(list(range(train_dataloader.__len__()))),\n",
        "                             drop_last=False)\n",
        "val_loader = DataLoader(val_dataloader,batch_size=batch_size,shuffle=False,\n",
        "                              num_workers=0,\n",
        "                              sampler=SubsetRandomSampler(list(range(len(val_dataloader.dataset)))),\n",
        "                             drop_last=False)\n",
        "\n",
        "best_loss = None\n",
        "raw_line0 = r'''Epoch[{}]    |    Lr:{}'''\n",
        "raw_line1 = r'''TOTAL Train loss: {}  |  TOTAL Val loss: {}  |  Time:{:.1f} min '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnvSqcWojObq"
      },
      "source": [
        " path_test1 = get_filepaths(\"/content/drive/MyDrive/Induction_training_files/test\")\n",
        " dp = pd.DataFrame({\"raw\":path_test1})\n",
        " dp.to_csv(\"qiux .csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGxSmVHY3o2c",
        "outputId": "4fd933bd-95ff-4f46-ed18-a9d653290f63"
      },
      "source": [
        "for epoch in range(1, EPOCHES+1):\n",
        "        losses = []\n",
        "        start_time = time.time()\n",
        "        t = tqdm(train_loader)\n",
        "        model.train()\n",
        "        for i,(input_image,gt_image) in enumerate(t):\n",
        "            input_image,gt_image = input_image.to(device), gt_image.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(input_image)\n",
        "            loss = loss_fn(y_pred,gt_image)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        vloss = validation(model, val_loader, loss_fn)\n",
        "        print(raw_line0.format(epoch,optimizer.param_groups[0][\"lr\"]))\n",
        "        print(raw_line1.format(np.array(losses).mean(),vloss,(time.time()-start_time)/60**1))\n",
        "\n",
        "        if best_loss == None:\n",
        "            best_loss = vloss\n",
        "            T.save(model.state_dict(), 'model2.pth')\n",
        "            print(\"saving model ..\")\n",
        "        if vloss < best_loss:\n",
        "            best_loss = vloss\n",
        "            T.save(model.state_dict(), 'model2.pth')\n",
        "            print(\"saving model ..\")\n",
        "        scheduler.step(vloss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "100%|██████████| 28/28 [00:54<00:00,  1.96s/it]\n",
            "100%|██████████| 28/28 [00:43<00:00,  1.57s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[1]    |    Lr:0.001\n",
            "TOTAL Train loss: 0.15714856263782298  |  TOTAL Val loss: 0.011600439569779806  |  Time:1.6 min \n",
            "saving model ..\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.51s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[2]    |    Lr:0.001\n",
            "TOTAL Train loss: 0.021505268955869333  |  TOTAL Val loss: 0.008784700683983309  |  Time:1.6 min \n",
            "saving model ..\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:52<00:00,  1.86s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.51s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[3]    |    Lr:0.001\n",
            "TOTAL Train loss: 0.020879468681024655  |  TOTAL Val loss: 0.008048632930565094  |  Time:1.6 min \n",
            "saving model ..\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:52<00:00,  1.86s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.52s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[4]    |    Lr:0.001\n",
            "TOTAL Train loss: 0.01993614262236016  |  TOTAL Val loss: 0.00831447496810662  |  Time:1.6 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:52<00:00,  1.86s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.52s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[5]    |    Lr:0.001\n",
            "TOTAL Train loss: 0.01936420804954001  |  TOTAL Val loss: 0.008545484942650157  |  Time:1.6 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.85s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.51s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[6]    |    Lr:0.001\n",
            "TOTAL Train loss: 0.018667703328121985  |  TOTAL Val loss: 0.011848553749067443  |  Time:1.6 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:52<00:00,  1.86s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.51s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[7]    |    Lr:0.001\n",
            "TOTAL Train loss: 0.01780614785717002  |  TOTAL Val loss: 0.01487548665941826  |  Time:1.6 min \n",
            "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[8]    |    Lr:0.0001\n",
            "TOTAL Train loss: 0.016821500146761537  |  TOTAL Val loss: 0.011290975214381303  |  Time:1.6 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.85s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.48s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[9]    |    Lr:0.0001\n",
            "TOTAL Train loss: 0.015991096584392444  |  TOTAL Val loss: 0.010757629892655782  |  Time:1.6 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.49s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[10]    |    Lr:0.0001\n",
            "TOTAL Train loss: 0.01578159267748041  |  TOTAL Val loss: 0.010011495091021061  |  Time:1.5 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.85s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.51s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[11]    |    Lr:0.0001\n",
            "TOTAL Train loss: 0.015662360869880234  |  TOTAL Val loss: 0.010976669411840183  |  Time:1.6 min \n",
            "Epoch    11: reducing learning rate of group 0 to 1.0000e-05.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.85s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.48s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[12]    |    Lr:1e-05\n",
            "TOTAL Train loss: 0.0155283457133919  |  TOTAL Val loss: 0.0106975249280887  |  Time:1.6 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.47s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[13]    |    Lr:1e-05\n",
            "TOTAL Train loss: 0.015447905619761773  |  TOTAL Val loss: 0.010725544765591621  |  Time:1.5 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:50<00:00,  1.82s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.48s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[14]    |    Lr:1e-05\n",
            "TOTAL Train loss: 0.015493953766833459  |  TOTAL Val loss: 0.010630737291648984  |  Time:1.5 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.82s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.49s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[15]    |    Lr:1e-05\n",
            "TOTAL Train loss: 0.015397862564506275  |  TOTAL Val loss: 0.010595480745126094  |  Time:1.5 min \n",
            "Epoch    15: reducing learning rate of group 0 to 1.0000e-06.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.49s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[16]    |    Lr:1.0000000000000002e-06\n",
            "TOTAL Train loss: 0.015295503128852164  |  TOTAL Val loss: 0.010644444530563695  |  Time:1.6 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.82s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.48s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[17]    |    Lr:1.0000000000000002e-06\n",
            "TOTAL Train loss: 0.015376773263726915  |  TOTAL Val loss: 0.010524669063410588  |  Time:1.5 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.47s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[18]    |    Lr:1.0000000000000002e-06\n",
            "TOTAL Train loss: 0.015245285350829363  |  TOTAL Val loss: 0.010374382537390505  |  Time:1.5 min \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:50<00:00,  1.82s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.52s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[19]    |    Lr:1.0000000000000002e-06\n",
            "TOTAL Train loss: 0.01531283840137933  |  TOTAL Val loss: 0.011218236559735877  |  Time:1.6 min \n",
            "Epoch    19: reducing learning rate of group 0 to 1.0000e-07.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.86s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[20]    |    Lr:1.0000000000000002e-07\n",
            "TOTAL Train loss: 0.015367623817707812  |  TOTAL Val loss: 0.010729645711502858  |  Time:1.6 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:52<00:00,  1.87s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[21]    |    Lr:1.0000000000000002e-07\n",
            "TOTAL Train loss: 0.015236233826726675  |  TOTAL Val loss: 0.01068974971505148  |  Time:1.6 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[22]    |    Lr:1.0000000000000002e-07\n",
            "TOTAL Train loss: 0.015267557564324566  |  TOTAL Val loss: 0.01046510779165796  |  Time:1.5 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[23]    |    Lr:1.0000000000000002e-07\n",
            "TOTAL Train loss: 0.015419922369931425  |  TOTAL Val loss: 0.010333385385040725  |  Time:1.5 min \n",
            "Epoch    23: reducing learning rate of group 0 to 1.0000e-08.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[24]    |    Lr:1.0000000000000004e-08\n",
            "TOTAL Train loss: 0.01535501387635512  |  TOTAL Val loss: 0.010527577517288072  |  Time:1.6 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.85s/it]\n",
            "100%|██████████| 28/28 [00:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[25]    |    Lr:1.0000000000000004e-08\n",
            "TOTAL Train loss: 0.015359173262757915  |  TOTAL Val loss: 0.010580704946603094  |  Time:1.6 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.84s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[26]    |    Lr:1.0000000000000004e-08\n",
            "TOTAL Train loss: 0.015394799278250762  |  TOTAL Val loss: 0.01078134442546538  |  Time:1.5 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:50<00:00,  1.82s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[27]    |    Lr:1.0000000000000004e-08\n",
            "TOTAL Train loss: 0.015370718203485012  |  TOTAL Val loss: 0.01055990209403847  |  Time:1.5 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.83s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[28]    |    Lr:1.0000000000000004e-08\n",
            "TOTAL Train loss: 0.015287261256682021  |  TOTAL Val loss: 0.010507834177198154  |  Time:1.5 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:51<00:00,  1.84s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[29]    |    Lr:1.0000000000000004e-08\n",
            "TOTAL Train loss: 0.01538237972584154  |  TOTAL Val loss: 0.010854362942544478  |  Time:1.5 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:50<00:00,  1.82s/it]\n",
            "100%|██████████| 28/28 [00:41<00:00,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[30]    |    Lr:1.0000000000000004e-08\n",
            "TOTAL Train loss: 0.015324107969978027  |  TOTAL Val loss: 0.010781428876465984  |  Time:1.5 min \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlCt9dyh9c0O"
      },
      "source": [
        "class Data2(Dataset):\n",
        "    def __init__(self, dir_=None, input_size=(224,224), output_size=(112, 112),prob = 0.15):\n",
        "        super().__init__()\n",
        "        self.directory = dir_\n",
        "        self.dataset = pd.read_csv(self.directory)\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.prob = prob\n",
        "        self.input_size_x = self.input_size[0]\n",
        "        self.input_size_y = self.input_size[1]\n",
        "        self.MODEL_SCALE = self.input_size[0]//self.output_size[0]\n",
        "        self.preprocess = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.dataset)\n",
        "\n",
        "    def __getitem_internal__(self, idx, preprocess=True):\n",
        "        target = self.dataset.iloc[idx]\n",
        "        noise_image = cv2.imread(target[\"raw\"])\n",
        "        dimensions = noise_image.shape\n",
        "        noise_image = cv2.resize(noise_image,self.input_size)\n",
        "        noise_image = transforms.ToTensor()(np.array(noise_image))\n",
        "        basename1 = os.path.basename(target[\"raw\"])\n",
        "        return (noise_image,basename1,dimensions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.__getitem_internal__(idx, True)\n",
        "    \n",
        "    def raw(self, idx):\n",
        "        return self.__getitem_internal__(idx, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2uK7M4q1Wmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f15bd9-49f6-4815-eb2b-b72bf1eec89d"
      },
      "source": [
        "test_dataloader = Data2(\"/content/qiux .csv\")\n",
        "print(\"Test :\",test_dataloader.__len__())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test : 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih815qcH1zVN"
      },
      "source": [
        "import os\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage import io\n",
        "\n",
        "def evaluate(p_img,gt_img):\n",
        "    s = ssim(p_img, gt_img, multichannel=True)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4myydfAPf_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a74efd-2527-4fc0-d7d5-63501bd1be0c"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "model.load_state_dict(T.load(\"/content/model2.pth\",map_location=T.device('cuda')))\n",
        "model.eval()\n",
        "ssim_t = []\n",
        "for i in range(60):\n",
        "    raw_normalized ,basename1 , dimension = test_dataloader[i]\n",
        "    \n",
        "    y_pred = model(raw_normalized.unsqueeze(dim=0))\n",
        "    raw_normalized = raw_normalized.permute(1,2,0).numpy()\n",
        "    y_pred = y_pred.squeeze(dim=0).permute(1,2,0).detach().cpu().numpy()\n",
        "    path1 = os.path.join('/content/drive/MyDrive/ai enhanced validation imagres', basename1)\n",
        "    y_pred = cv2.resize(y_pred, (dimension[1],dimension[0]))\n",
        "    y_pred = cv2.convertScaleAbs(y_pred,alpha=(255.0))\n",
        "    cv2.imwrite(path1,y_pred)\n",
        "    #plt.imshow(y_pred)\n",
        "    #plt.savefig(path1)\n",
        "    #plt.show()\n",
        "    #fig, ax = plt.subplots(1, 2,figsize=(16,4))\n",
        "    #ax[0].imshow(raw_normalized)        \n",
        "    #ax[0].set_xticks([])\n",
        "    #ax[0].set_yticks([])\n",
        "    #ax[1].imshow(y_pred)        \n",
        "    #ax[1].set_xticks([])\n",
        "    #ax[1].set_yticks([])\n",
        "    #plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ]
        }
      ]
    }
  ]
}